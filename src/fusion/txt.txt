## First we need a motion estimation model for a mobile robot or autonomous vehicle
# for 50hz vehicle not faster! and small change in the heading angle (constant heading rate)
theta_1 = thetat_0 + delta_theta_1
x_1 = x_0 + delta_s_1 * cost(theta_0 + delta_theta_1/2)
y_1 = y_0 + delta_s_1 * sin(theta_0 + delta_theta_1 /2)
delta_s = v_x * delta_t
delta_theta = theta_dot * delta_t


this was from a youtube video
I remeber i used this for a mobile robot once in matlab

x1 = x0 + v*cos(theta)*delta_t
y1 = y0 + v*sin(theta)*delta_t
theta = theta_0 + omega*delta_t


more sophistaced kinemtatics can be utilized if you want more accuracy




TODO: Need to use real time library for the time -->but why ?is there a time constrait?! is there a callback thats being called every 10 ms and you have to fill the buffer in this time constraint!? i don't think so!

should i make distinction between ekf2d 3d . should i create a state space class and inject it into motion model !


take a look at this --> https://robotics.stackexchange.com/questions/23179/sensor-fusion-with-extended-kalman-filter-for-roll-and-pitch
this was for fusing accelerometer with the gyroscoe to get the roll and pitch data! --> but doesn't the imu already provide those?! maybe its not using the acceleration and it only integrate the angular velocities!

https://dsp.stackexchange.com/questions/8860/kalman-filter-for-position-and-velocity-introducing-speed-estimates
https://towardsdatascience.com/kalman-filter-an-algorithm-for-making-sense-from-the-insights-of-various-sensors-fused-together-ddf67597f35e

-------------
motion model for constant heading rate (v and w are constants)
x_dot = v*cos(yaw) = f1
y_dot = v*sin(yaw) = f2
yaw_dot = w = f3


in continuous mode  with the state space like this : [x;y;yaw;v;w]
A = [df1/dx , df1/dy , df1/dYaw , df1/dv , df1/dw
    df2/dx , df2/dy , df2/dYaw , df2/dv , df2/dw
    df3/dx , df3/dy , df3/dYaw , df3/dv , df3/dw
    df4/dx ....
    df5/dx ....] //albate f4 va f5 nadarim dar vaghe v=v va w=w mishe f4 va f5

= [0 0 -v*sin(yaw) cos(yaw) 0
   .  . .. . . . .. . . 
   . .. . . . .. . . . .
   ................
   ................. ]



in discrete mode:
x(k+1) = x(k) + v*cos(yaw(k))*dt =f1
y(k+1) = y(k) + v*sin(yaw(k))*dt = f2
yaw(k+1) = yaw(k) + w*dt  =f3
v(k+1) = v(k) =f4 --> because of my choosen state space in kf i have to write this also
w(k+1) = w(k) =f5 ---> same as above line

A = [df1/dx(k) , df1/dy(k) , df1/dYaw(k) , df1/dv(k) , df1/dw(k)
df2/dx(k) , df2/dy(k) , df2/dYaw(k) , df2/dv(k) , df2/dw(k)
df3/dx(k) , df3/dy(k) , df3/dYaw(k) , df3/dv(k) , df3/dw(k)
df4/dx(k) , df4/dy(k) , df4/dYaw(k) , df4/dv(k) , df4/dw(k)
df5/dx(k) , df5/dy(k) , df5/dYaw(k) , df5/dv(k) , df5/dw(k)
]

so the A would be :

A= [ 1 0 -v*sin(yaw(k))*dt   cos(yaw(k))*dt  0
     0 1  v*cos(yaw(k))*dt   sin(yaw(k))*dt  0
     0 0        1                 0          dt
     0 0        0                 1          0
     0 0        0                 0          1]

state space :
X = [x 
     y
     yaw 
     v 
     w]

TODO: have solid reasons for all your TODOs
TODO: put these in the motion model of constant heading rate not those formula! but right now im gonna convert them in the ekf predict function
TODO: i also didn't implement measurement model yet!
TODO: use observer design so that as soon as cmd_vel or any other velocity source  has changed it would notify the motion model
TODO: P0 zero should me inf if you don't know where the robot is at first iteration. for now I assume we know the first location because im working on tracking not localization
TODO: create a factory for ekf and stuff
TODO: should i make the fusion template not the ekf or what?
TODO: normalize yaw angle in constion heading rate
TODO: create state space class and not just create a matrix in the ekf!
TODO: when do i know the measurment has arrived and that I should do a ekk update! for now i just go with it assuming I have the Imu measurements
TODO: You need to update the motion models states based on the output of the ekf's update and set velocity based on cmd is wrong now that you updated the states
TODO: EIgen auto differentation for jacobain in motion model --> use the hard code for the firstt 3by3 mat and for other based on the states make the diffs
TODO: take care of Q matrix . it shouldn't keep goind up if we are not moving I guess!
TODO: propgate equation in motion model needs reconsideration. mainly motion model velocity commands may get updated by the output of the ekf if its being updated by or Not. mind the jacobian. changing the equation messes up the jacobian
TODO: https://automaticaddison.com/sensor-fusion-using-the-robot-localization-package-ros-2/ ---> test it with your pkg and robot_locaziation ros pkg
TODO: check the config file of ekf.yaml and get ideas --> another thing:https://github.com/cra-ros-pkg/robot_localization/issues/249 , another thing: have the full states specified but in the matricies put zero (i guess H)
TODO: make it so that the update of ekf happens if the meas data is available
TODO: the observation sources shouldn't be in the ekf templates! it should be optional so if we have an observation it'd correct the states else we only have predcition step
TODO: changed the api of ekf and added setMotionModel instead of gettting it in the constructor
TODO: created the filter node to get the parameters of the sensor topics and states. so create a class memebr var to utilize them. aslo finish the fusion factory and also figure out a elognat way to communicate the sensors data without the template thing
TODO: enqueu the observations
TODO: make the H matrix like a num_states*num_states matrix then make some of the diags 1 based on the sensors state and then createa full states observation and make the one that you don't zero --> this way the states that is not being observed won't get updated
TODO: each specific measurement maybe should have its own H matrix instead of creating a constant H matrix and just make it zero and one based on what state the sensor provides Or not!
TODO: inverse of a matrix with zero on its diagonal would be nan i used a work around and defined R to be fully diag but thats not correct. find a way to just find an inverse of a sub matrix using eigen
TODO: also now you don't have cmd_vel subscriber! add it or go with the odom --> you can use use_control config file to subscribe to cmd_vel and then use the mediator to communicate!
TODO: bad thing about map is you might accidentally add to it! like for example in the getState of ekf so the best thing to do is either do not depend on map size or make the getstateorder func in state space a copy not a reference!
TODO: take care of adding or removing a state from the config file right now i removed x_ddot!
      Ekf is template base so the test_ekf.cpp would give me errors on creating ekf because ekf.hpp is not visiible ---> Template classes need to have their implementation visible during compilation
TODO: using the constant acc for velocity estimation and using imu is yeilding bad result due to not being able to stop the robot after giving stop command because acc is 0 when the robot is mvoing with constant vel and in rest so distingusing between the two is important 
      maybe i can use the cmd_vel topic just
TODO: maybe i should make some codes in the callbacks async. maybe use a timer for them
TODO: implement logger()
TODO: maybe visulization should be on another thread (in the timer of the filter node!)
TODO: take care of Q and R and also add intial position for the states
TODO: error handling!
TODO: how to get rid of too many data in the queue? should i delete the old data faster thank the current rate because its highly depandant on the filters rate!
TODO: implement the odom topic. relying only on imu is not gonna work
TODO: Its unstable
TODO: fix the delay using https://answers.ros.org/question/221036/robot_localization-lag/ --> weridly enough if you show odometry in rviz its fast in use sim time false mode but the tf seems to be ahead of time!
TODO: gazebo clock speed which is 10 gonna hurt your simulation --> https://answers.ros.org/question/391570/robot-localization-ekf-capped-at-10hz/  AND https://github.com/cra-ros-pkg/robot_localization/pull/615
TODO: I tested it with a bag file test1 and test2 in rl pkg and it works ok so lets not waste anymore time on the lag issue. but another way to be sure is to use it in an slam pkg like cartogrpher as an odometry source instead of the real odom plugin and rotate in place to see if there is any lag or something
TODO: I can get the jacobian. create a way for people to create their own motion model and you take the jacobian yourself based on what they wrote
TODO: i think a(k+1) = a(k) is not correct is it because it means constant acceleration which clearly is so may be increase the Q on this element so that it trust the measurement 
TODO: whats the point of the prediction if its in the observation not empty if condition. i want it in case the measurement are having trouble getting to the filter. but at the same time i don't want the P cov to explode! so when P is huge the estimation is unreliable so may be i can add sensor_time out like for 10 seconds and that the limit it can do without having measurment. this is 
      double edged sword because the model is gonna predict based on the last measurement information
TODO: get rid of eigen and use only autodiff lib. is it possible?

TODO: you used amet_target export depednenices checkl if the simple model cmake can se this. so remove eigen3 and autodiff see what happens
TODO: initial possition and Q and initial covairance of P set it in the param. put covariance for the odom_filetered topic

solution --> 
Option 1: Move Entire Template Implementation to Header
Option 2: Explicitly Instantiate Templates at the cpp file -->template class Ekf<geometry_msgs::msg::Twist, sensor_msgs::msg::Imu>;
Option 3: Include ekf.cpp in Test File -->not common practice




for Eigen jacobian --> https://joelcfd.com/automatic-differentiation/
or https://autodiff.github.io/tutorials/


Note: one thing to note is that the jacobian at the prediction step is calculated using the previous states! so if you are updating them then using the hardcoded jacobian matrix you are doing it wrong! so better approach is to use the 
      jacobian function in autodiff which calcs the jacobian using the current state and at the same time propagate the state one time interval forward

autodiff probelm with member class functions for jacobian--> https://github.com/autodiff/autodiff/issues/51
important thing about the jacobian is when you update the states after the jacobian functino you have to do it one by one and USE val() function or else it gives wrong jacobina calculations!

https://answers.ros.org/question/370980/3d-pose-with-imu-and-odometry-using-robot-localization/




gdb : 
https://juraph.com/miscellaneous/ros2_and_gdb/




for debuging run this in the terminal:
first build the project : colcon build --cmake-args -DCMAKE_BUILD_TYPE=Debug
ros2 run --prefix  'gdbserver localhost:3000'  fusion fusion --ros-args --params-file src/fusion/params/ekf.yaml
and then go the main.cpp and hit play button


or for a direct gdb:
ros2 run --prefix 'gdb -ex run --args' fusion fusion --ros-args --params-file src/fusion/params/ekf.yaml   

-----------
make the unordered map const and here is a conversation between me and chatgpt:
"
why index.at("x_dot") works but index["x_dot"] doens't???
mind you the index is const unordered map
ChatGPT
Ah, I see where the confusion lies. The difference between index.at("x_dot") and index["x_dot"] in a const unordered map lies in how they handle non-existing keys.

index.at("x_dot"): This method is safe and will throw an std::out_of_range exception if the key doesn't exist in the map. This is good for explicitly handling cases where the key might not be present.

index["x_dot"]: This method is not available in a const-qualified context of the unordered map. In a const map, operator[] is disabled for inserting or modifying elements because it can potentially create a new element if the key doesn't exist, which isn't allowed in a const-qualified context.
"

kholase inke vaghti const ref bargardoni az getStateOrder bayad az index.at use kuni chun in safe hast ke age nabood chizi ijad nakune! --> compiler's intelligence leaves at awe sometimes!


good info on only imu --> https://answers.ros.org/question/232794/can-i-use-robot_localization-package-with-only-imu-data-if-i-couldhow-to-config-the-package/
so if you wanna use only imu you should add bias error as state!

info regarding lidar localization ==> also really good debuging approach is in there
https://github.com/cra-ros-pkg/robot_localization/issues/370


regarding stability of ekf:
https://answers.ros.org/question/373453/robot_localization-imu-yaw-velocity-causes-vertical-drift/


simple overview of rl pkg :
https://answers.ros.org/question/372501/robot-localization-time-stamps/
https://answers.ros.org/question/220538/robot_localization-time-issues/

rl publish rate:
https://github.com/cra-ros-pkg/robot_localization/issues?q=publish+rate











A problem whith the pi to pi angles: see the growth in yaw_dot estimation! when you don't provide a measurement for it!

[INFO] [1704634016.409077827] [Filter]: dt: 0.036641
[INFO] [1704634016.409321155] [Filter]: YAW IN ODOM -3.125029
[INFO] [1704634016.425103141] [Filter]: YAW IN ODOM -3.134004
real ---: 
-0.000172503
  0.00109223
    -3.12503
 8.47553e-05
           0
           0
X_BEFORE: 
-0.000169652
  0.00109262
    -3.11392
 8.82816e-05
    0.134111
 0.000127683
X: 
-0.000172503
  0.00109223
    -3.12503
 8.47553e-05
    0.124609
 0.000124668
[INFO] [1704634016.428546729] [Filter]: dt: 0.016147
real ---: 
-0.000173055
  0.00109224
      -3.134
 7.23185e-05
           0
           0
X_BEFORE: 
-0.000172507
  0.00109223
    -3.12502
 8.47615e-05
    0.124609
 0.000124668
X: 
-0.000173055
  0.00109224
      -3.134
 7.23185e-05
    0.124585
 0.000124634
[INFO] [1704634016.431601605] [Filter]: dt: 0.000050
[INFO] [1704634016.445043500] [Filter]: YAW IN ODOM 3.139337
real ---: 
-0.000177266
  0.00109172
     3.13934
 8.42112e-05
           0
           0
X_BEFORE: 
-0.000174037
  0.00109223
    -3.13231
  7.4011e-05
    0.124585
 0.000124634
X: 
-0.000177266
  0.00109172
     3.13933
 8.42112e-05
       4.758
  0.00013217
[INFO] [1704634016.448035602] [Filter]: dt: 0.013580
[INFO] [1704634016.465048648] [Filter]: No obs is in queue 0
[INFO] [1704634016.465197424] [Filter]: YAW IN ODOM 3.128595
[INFO] [1704634016.485049814] [Filter]: YAW IN ODOM 3.117826
real ---: 
-0.00018182
 0.00109122
     3.1286
7.94497e-05
          0
          0
X_BEFORE: 
-0.00018039
 0.00109173
   -2.96734
8.91145e-05
      4.758
 0.00013217
X: 
-0.00018182
 0.00109122
    3.12859
7.94497e-05
    16.3999
0.000113712
[INFO] [1704634016.487715055] [Filter]: dt: 0.037099
real ---: 
-0.000182421
  0.00109125
     3.11783
 7.48636e-05
           0
           0
X_BEFORE: 
-0.000181823
  0.00109122
     3.12924
 7.94542e-05
     16.3999
 0.000113712
X: 
-0.000182421
  0.00109125
     3.11783
 7.48636e-05
     16.3999
 0.000113703
[INFO] [1704634016.490854345] [Filter]: dt: 0.000040
[INFO] [1704634016.505084545] [Filter]: YAW IN ODOM 3.106167
real ---: 
-0.000179651
  0.00109108
     3.10617
 -0.00012973
           0
           0
X_BEFORE: 
-0.000183496
  0.00109128
    -2.92991
  7.6496e-05
     16.3999
 0.000113703
X: 
-0.000179651
  0.00109108
     3.10616
 -0.00012973
     20.9837
 -4.2906e-05
[INFO] [1704634016.509568141] [Filter]: dt: 0.014357
[INFO] [1704634016.525053221] [Filter]: YAW IN ODOM 3.093628
real ---: 
-0.00018436
 0.00109048
    3.09363
 8.9316e-05
          0
          0
X_BEFORE: 
-0.000177633
  0.00109101
    -2.85032
-0.000130398
     20.9837
 -4.2906e-05
X: 
-0.00018436
 0.00109048
    3.09362
8.93159e-05
    25.9071
0.000139085




ros2 pkg create --build-type ament_cmake model --dependencies autodiff Eigen3 fusion pluginlib --library-name model
https://docs.ros.org/en/galactic/Tutorials/Beginner-Client-Libraries/Pluginlib.html




whats wrong with this piece of code:

        if(!model_plugin.empty())
        {
            std::shared_ptr<MotionModel> load = model_factory_->createModelFromPlugin(model_plugin, states_);
            model_ = std::unique_ptr<MotionModel>(load.get());
            std::cout<<"Model Address 1: "<<model_.get()<<"\n";
        }

The problem here is that load is a std::shared_ptr<MotionModel>, and when you call load.get(), you get a raw pointer (MotionModel*) pointing to the object. Then, when you use std::unique_ptr<MotionModel>(load.get()), you create a new std::unique_ptr that thinks it owns the object, but the original std::shared_ptr still owns it. When the std::shared_ptr goes out of scope or is explicitly destroyed, it deletes the object, leaving the std::unique_ptr with a dangling pointer, leading to undefined behavior.

kholase you can not convert shared_ptr to unique ptr but vice versa is possible




what should be the Q: https://stackoverflow.com/questions/36736376/kalman-filter-process-noise-covariance
basically if your real dynamics are not the same as the motion model that you are using the Q is gonna compensate for it by the amount you are lacking! good example is in the above link
